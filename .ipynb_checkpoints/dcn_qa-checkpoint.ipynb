{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(filepath,ml):\n",
    "    c = []\n",
    "    if ml == 0:\n",
    "        ml = 1000\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            s = 0\n",
    "            c.append([int(line.split()[i]) for i in range(len(line.split())) if i<ml])\n",
    "    return c\n",
    "\n",
    "def final_preprocess(c):   #padding\n",
    "    max_l = max([len(i) for i in c])\n",
    "    for i in range(len(c)):\n",
    "        c[i] = c[i] + (max_l-len(c[i]))*[0]\n",
    "    return c,max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading train data and preprocessing them + padding\n",
    "#pad the context and question data\n",
    "context_data,max_l_context = final_preprocess(preprocess('data/train.ids.context',600))\n",
    "question_data,max_l_question = final_preprocess(preprocess('data/train.ids.question',0))\n",
    "answer_data = preprocess('data/train.span',0)\n",
    "answer_start_data = [i[0] for i in answer_data]\n",
    "answer_end_data = [i[1] for i in answer_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print max_l_context\n",
    "print max_l_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))\n",
    "    length = tf.reduce_sum(used, axis=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    #print \"sequence length tf shape:\",length.shape\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder function\n",
    "def encoder(question,context,embeddings,hidden_units=200):\n",
    "    batch_size = tf.shape(question)[0]\n",
    "    #question and document encoder\n",
    "    q_embedding = tf.nn.embedding_lookup(embeddings,question)\n",
    "    d_embedding = tf.nn.embedding_lookup(embeddings,context)\n",
    "    \n",
    "    print q_embedding.shape,\"=?,60,100\"\n",
    "    print d_embedding.shape,\"=?,600,100\"\n",
    "    \n",
    "    lstm_enc = tf.contrib.rnn.BasicLSTMCell(hidden_units)\n",
    "    \n",
    "    with tf.variable_scope('document_encoder') as scope1:\n",
    "        document_states,_ = tf.nn.dynamic_rnn(cell=lstm_enc,\n",
    "                                              dtype=tf.float32,\n",
    "                                              inputs=d_embedding,\n",
    "                                              sequence_length=length(q_embedding),\n",
    "                                              time_major=False)\n",
    "        \n",
    "    with tf.variable_scope('question_encoder') as scope2:\n",
    "        question_states,_ = tf.nn.dynamic_rnn(cell=lstm_enc,\n",
    "                                              dtype=tf.float32,\n",
    "                                              inputs=q_embedding,\n",
    "                                              sequence_length=length(d_embedding),\n",
    "                                              time_major=False)\n",
    "\n",
    "    Wq = tf.get_variable(name=\"Wq\",shape=[hidden_units,hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    bq = tf.Variable(tf.constant(0.0,shape=[hidden_units,]),dtype=tf.float32,name='bq')\n",
    "    Wq = tf.expand_dims(tf.ones([batch_size,1]), 1) * Wq\n",
    "    #question_states_new = tf.reshape(question_states,shape=[-1,hidden_units])\n",
    "    print document_states.shape,\"=?,600,200\"\n",
    "    print question_states.shape,\"=?,60,200\"\n",
    "    #print question_states_new.shape,\"=?,200\"\n",
    "    print Wq.shape,\"=?,200,200\"\n",
    "    print bq.shape,\"=200,\"\n",
    "    \n",
    "                    \n",
    "    question_states_modified_duplicate = tf.nn.tanh(tf.matmul(question_states,Wq)+bq)\n",
    "    #question_states_modified_duplicate = tf.reshape(question_states_modified,shape=[-1,int(question_states.shape[1]),hidden_units])\n",
    "    question_states_modified = tf.transpose(question_states_modified_duplicate,perm=[0,2,1]) #tf.reshape(question_states_modified,shape=[-1,hidden_units,int(question_states.shape[1])])\n",
    "    print question_states_modified.shape,\"=?,200,60\"\n",
    "    print question_states_modified_duplicate.shape,\"=?,60,200\"\n",
    "\n",
    "    #coattention encoder\n",
    "    \n",
    "    l = tf.matmul(document_states,question_states_modified)\n",
    "    print l.shape,\"=?,600,60\"\n",
    "    aq = tf.nn.softmax(l)\n",
    "    ad = tf.nn.softmax(tf.transpose(l,perm=[0, 2, 1]))\n",
    "    print aq.shape,\"=?,600,60\"\n",
    "    print ad.shape,\"=?,60,600\"\n",
    "    \n",
    "    cq = tf.matmul(tf.transpose(aq,perm=[0,2,1]),document_states)\n",
    "    print cq.shape,\"=?,60,200\"\n",
    "    print question_states_modified_duplicate.shape,\"=?,60,200\"\n",
    "    qcq = tf.concat([question_states_modified_duplicate,cq],2)\n",
    "    print qcq.shape,\"=?,60,400\"\n",
    "    cd = tf.matmul(tf.transpose(ad,perm=[0,2,1]),qcq)\n",
    "    print cd.shape,\"=?,600,400\"\n",
    "    dcd = tf.concat([document_states,cd],axis=2)\n",
    "    \n",
    "    with tf.variable_scope('coattention'):\n",
    "        u_lstm_fw = tf.contrib.rnn.BasicLSTMCell(hidden_units)  #bi-lstm\n",
    "        u_lstm_bw = tf.contrib.rnn.BasicLSTMCell(hidden_units)\n",
    "        u_states,_ = tf.nn.bidirectional_dynamic_rnn(cell_bw=u_lstm_bw,cell_fw=u_lstm_fw,dtype=tf.float32,inputs=dcd,time_major=False,sequence_length=length(dcd))\n",
    "    encoder_states = tf.concat(u_states,2)\n",
    "    print encoder_states.shape\n",
    "    return encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder function\n",
    "def decoder(knowledge_reps,hidden_units = 200):\n",
    "    #randomly initialise s and e\n",
    "    batch_size = tf.shape(knowledge_reps)[0]\n",
    "    #print batch_size\n",
    "    pool = 16\n",
    "    e = np.random.randint(max_l_context) + 1\n",
    "    s = np.random.randint(e)\n",
    "    sv = tf.tile([s],[batch_size])\n",
    "    ev = tf.tile([e],[batch_size])\n",
    "\n",
    "    #lstm cell\n",
    "    #with tf.variable_scope('lstm_dec') as scope_dec:\n",
    "    lstm_dec = tf.contrib.rnn.LSTMCell(hidden_units)\n",
    "    ch = lstm_dec.zero_state(batch_size,dtype=tf.float32)\n",
    "    hi,ci = ch\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('hmn1') as scope1:\n",
    "        wd = tf.get_variable(name=\"wd\",shape=[hidden_units,5*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w1 = tf.get_variable(name=\"w1\",shape=[pool*hidden_units,3*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w2 = tf.get_variable(name=\"w2\",shape=[pool*hidden_units,hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w3 = tf.get_variable(name=\"w3\",shape=[pool,2*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('hmn2') as scope2:\n",
    "        wd = tf.get_variable(name=\"wd\",shape=[hidden_units,5*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w1 = tf.get_variable(name=\"w1\",shape=[pool*hidden_units,3*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w2 = tf.get_variable(name=\"w2\",shape=[pool*hidden_units,hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        w3 = tf.get_variable(name=\"w3\",shape=[pool,2*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "\n",
    "        \n",
    "    #loop 4 times to call lstm cell to:\n",
    "    for i in range(4):\n",
    "        #concatenate u_s and u_e\n",
    "        u_s = tf.gather_nd(params=knowledge_reps,indices=tf.stack([tf.range(batch_size,dtype=tf.int32),sv],axis=1))\n",
    "        u_e = tf.gather_nd(params=knowledge_reps,indices=tf.stack([tf.range(batch_size,dtype=tf.int32),ev],axis=1))\n",
    "        usue = tf.concat([u_s,u_e],axis=1)\n",
    "        print i,usue.shape,hi.shape\n",
    "        #calculate hi\n",
    "         \n",
    "        with tf.variable_scope(\"hmn1\",reuse=True) as scope1:\n",
    "            sv,hmns_output = hmn(knowledge_reps,hi,u_s,u_e,hidden_units,pool)#loop over the document length times to obtain alpha t using HNM function\n",
    "        with tf.variable_scope(\"hmn2\",reuse=True) as scope2:\n",
    "            ev,hmne_output = hmn(knowledge_reps,hi,u_s,u_e,hidden_units,pool)#loop over the document length times to obtain beta t using HNM function\n",
    "        \n",
    "        hi,ch = lstm_dec(inputs=usue,state=ch) \n",
    "        \n",
    "    return sv,ev,hmns_output,hmne_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmn(kr,hs,us,ue,hidden_units,pool=16):\n",
    "    \n",
    "    #print \"kr\",kr.shape\n",
    "    #calculate r\n",
    "    wd = tf.get_variable(name=\"wd\",shape=[hidden_units,5*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    x = tf.concat([hs,us,ue],axis=1)\n",
    "    r = tf.nn.tanh(tf.matmul(x,tf.transpose(wd)))\n",
    "    #print r.shape\n",
    "\n",
    "    #calculate mt1\n",
    "    r1 = tf.expand_dims(tf.ones([int(kr.shape[1]),1]), 1) * r\n",
    "    r1 = tf.reshape(r1,[-1,hidden_units])\n",
    "    kr1 = tf.reshape(kr,[-1,2*hidden_units])\n",
    "    krr1 = tf.concat([kr1,r1],axis=1)\n",
    "    w1 = tf.get_variable(name=\"w1\",shape=[pool*hidden_units,3*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.constant(0.0,shape=[pool*hidden_units,]),dtype=tf.float32)\n",
    "    x1 = tf.matmul(krr1,tf.transpose(w1))+b1\n",
    "    x1 = tf.reshape(x1,[-1,pool])\n",
    "    x1 = tf.reduce_max(x1,axis=1)\n",
    "    m1 = tf.reshape(x1,[-1,hidden_units])\n",
    "    #print m1.shape\n",
    "    \n",
    "    #calculate mt2\n",
    "    w2 = tf.get_variable(name=\"w2\",shape=[pool*hidden_units,hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.constant(0.0,shape=[pool*hidden_units,]),dtype=tf.float32)\n",
    "    x2 = tf.matmul(m1,tf.transpose(w2))+b2\n",
    "    x2 = tf.reshape(x2,[-1,pool])\n",
    "    x2 = tf.reduce_max(x2,axis=1)\n",
    "    m2 = tf.reshape(x2,[-1,hidden_units])\n",
    "    #print m2.shape\n",
    "    \n",
    "    #max\n",
    "    m1m2 = tf.concat([m1,m2],axis=1)\n",
    "    #print \"m1m2\",m1m2.shape\n",
    "    w3 = tf.get_variable(name=\"w3\",shape=[pool,2*hidden_units],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "    b3 = tf.Variable(tf.constant(0.0,shape=[pool,]),dtype=tf.float32)\n",
    "    x3 = tf.matmul(m1m2,tf.transpose(w3))+b3\n",
    "    #print x3.shape\n",
    "    x3 = tf.reduce_max(x3,axis=1)\n",
    "    #print x3.shape\n",
    "    x3 = tf.reshape(x3,[-1,int(kr.shape[1])])\n",
    "    #print \"x3\",x3.shape\n",
    "    #argmax\n",
    "    output = tf.argmax(x3,axis=1)\n",
    "    output = tf.cast(output,dtype=tf.int32)\n",
    "    \n",
    "    return output,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read embedding file\n",
    "embedding_array = np.load('data/glove.trimmed.100.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 60, 100) =?,60,100\n",
      "(?, 600, 100) =?,600,100\n",
      "(?, 600, 200) =?,600,200\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 200, 200) =?,200,200\n",
      "(200,) =200,\n",
      "(?, 200, 60) =?,200,60\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 600, 60) =?,600,60\n",
      "(?, 600, 60) =?,600,60\n",
      "(?, 60, 600) =?,60,600\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 60, 400) =?,60,400\n",
      "(?, 600, 400) =?,600,400\n",
      "(?, 600, 400)\n",
      "decoder starts\n",
      "0 (?, 800) (?, 200)\n",
      "1 (?, 800) (?, 200)\n",
      "2 (?, 800) (?, 200)\n",
      "3 (?, 800) (?, 200)\n"
     ]
    }
   ],
   "source": [
    "## create placeholders\n",
    "tf.reset_default_graph()\n",
    "hidden_units = 200\n",
    "question = tf.placeholder(dtype=tf.int32,shape=[None,max_l_question])\n",
    "context = tf.placeholder(dtype=tf.int32,shape=[None,max_l_context])\n",
    "answer_start = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "answer_end = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "embeddings = tf.constant(embedding_array['glove'],dtype=tf.float32)\n",
    "\n",
    "encoder_states = encoder(question,context,embeddings)\n",
    "print \"decoder starts\"\n",
    "decoder_output_start, decoder_output_end, hmns_output, hmne_output = decoder(encoder_states)\n",
    "\n",
    "## add loss\n",
    "l1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=answer_start,logits=hmns_output)\n",
    "l2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=answer_end,logits=hmne_output)\n",
    "loss = l1 + l2\n",
    "## add optimizer\n",
    "train_op = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise variables and train\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 12.63499546  12.78450203  12.81515503  12.60778809  12.5678978\n",
      "  12.82588577  12.76771736  12.63602638  12.61898327  12.63025856\n",
      "  12.80628777  12.6170826   12.82214355  12.57518578  12.58350754\n",
      "  12.79389954  12.62486172  12.81266689  12.81122017  12.64863396\n",
      "  12.82440281  12.807827    12.66348839  12.82019234  12.65322208\n",
      "  12.57087708  12.80181789  12.56326389  12.80790806  12.81845284\n",
      "  12.68049622  12.59431076  12.81623077  12.58006573  12.70718956\n",
      "  12.82423973  12.53860855  12.57611656  12.69247723  12.81128693\n",
      "  12.60795307  12.59523582  12.788908    12.53867531  12.82902527\n",
      "  12.81334019  12.82305717  12.81127453  12.67887211  12.59577465\n",
      "  12.79367447  12.59959507  12.81070328  12.64161396  12.81628609\n",
      "  12.59114265  12.83227158  12.81814098  12.61690521  12.65913486\n",
      "  12.81230545  12.82863998  12.80570602  12.60024261  12.71194839\n",
      "  12.78351784  12.75302696  12.74301147  12.81563377  12.61703682\n",
      "  12.60290909  12.67138577  12.83285618  12.6493063   12.86676884\n",
      "  12.70110703  12.6404705   12.60547543  12.58703804  12.66031456\n",
      "  12.81228924  12.71889496  12.79559326  12.80985832  12.66751289\n",
      "  12.78477192  12.57977867  12.62600708  12.58662224  12.56894302\n",
      "  12.62225533  12.8085165   12.80073166  12.64480305  12.57224655\n",
      "  12.85291576  12.76267815  12.58587265  12.70775795  12.83754349\n",
      "  12.72364044  12.69685364  12.80079079  12.67551041  12.59728432\n",
      "  12.80693054  12.59950829  12.68812561  12.59150887  12.56624413\n",
      "  12.67735958  12.7933979   12.67327499  12.77294731  12.80076694\n",
      "  12.82569122  12.69813156  12.80896187  12.82331753  12.63747215\n",
      "  12.6175499   12.67404175  12.8251915   12.55965805  12.82409573\n",
      "  12.70253658  12.83566284  12.81787682  12.70824432  12.63191986\n",
      "  12.57643604  12.8383894   12.5555191   12.79731178  12.61555099\n",
      "  12.60450172  12.86393547  12.65380001  12.61036491  12.61992931\n",
      "  12.57314491  12.8098278   12.65257359  12.74172211  12.73313236\n",
      "  12.61657143  12.69056416  12.68741226  12.80118942  12.6111784\n",
      "  12.60726357  12.54728508  12.77264881  12.60161972  12.76704121\n",
      "  12.6258316   12.63833427  12.54603004  12.61705494  12.65921211\n",
      "  12.70656586  12.65684128  12.670578    12.66686058  12.68822098\n",
      "  12.80183411  12.77545929  12.64568424  12.57022858  12.71433067\n",
      "  12.5521822   12.80661488  12.84062099  12.8008213   12.59686089\n",
      "  12.59930229  12.59568405  12.65884876  12.64703751  12.81206322\n",
      "  12.83276176  12.53865433  12.77339363  12.81131172  12.53602028\n",
      "  12.66954899  12.73307419  12.59520721  12.55137634  12.59950066\n",
      "  12.65725517  12.76464081  12.57975578  12.53178024  12.63730526\n",
      "  12.77442741  12.83127785  12.77048492  12.58184242  12.81231976\n",
      "  12.78105068  12.82139778  12.60054398  12.62279034  12.86188698\n",
      "  12.60772133  12.79137802  12.59705353  12.66306114  12.80007362\n",
      "  12.62711334  12.58711243  12.80918884  12.69437504  12.81616402\n",
      "  12.79487991  12.62812805  12.82976151  12.5928936   12.79176712\n",
      "  12.80697441  12.60826492  12.68783188  12.8033886   12.83069324\n",
      "  12.7670927   12.76093864  12.64426422  12.81591606  12.61411572\n",
      "  12.83890343  12.80245113  12.80215645  12.61807823  12.6289444\n",
      "  12.66387939  12.83631802  12.5853405   12.57007027  12.82100677\n",
      "  12.65095139  12.78733063  12.6114502   12.57590675  12.70881653\n",
      "  12.59748745  12.58147907  12.58681774  12.81149483  12.60881042\n",
      "  12.59096146  12.56221294  12.65066719  12.6210041   12.86693192\n",
      "  12.80106449]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#session run train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_size = 256\n",
    "    for epocs in range(100):\n",
    "        counter = 0\n",
    "        for steps in range(318):\n",
    "            question_batch = np.array(question_data[counter:(counter+batch_size)])\n",
    "            context_batch = np.array(context_data[counter:(counter+batch_size)])\n",
    "            answer_start_batch = np.array(answer_start_data[counter:(counter+batch_size)])\n",
    "            answer_end_batch = np.array(answer_end_data[counter:(counter+batch_size)])\n",
    "            sess.run(train_op,feed_dict = {question : question_batch, context : context_batch, answer_start : answer_start_batch, answer_end : answer_end_batch})\n",
    "            print steps\n",
    "            if steps % 100 == 0:\n",
    "                loss_val = sess.run(loss,feed_dict = {question : question_batch, context : context_batch, answer_start : answer_start_batch, answer_end : answer_end_batch})\n",
    "                print loss_val\n",
    "            counter = counter + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read validation and test files\n",
    "#preprocess them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
